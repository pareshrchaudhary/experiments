{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World Environment formulized using MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorldEnv:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (3, 4) # gird world\n",
    "        self.start_state = (0, 0) # start state\n",
    "        self.goal_state = (2, 3)  # goal state\n",
    "        self.fire_state = (1, 3)  # fire state\n",
    "        self.null_space = (1, 1)  # null space\n",
    "        \n",
    "        self.step_reward = 0  # reward for each step\n",
    "        self.fire_penalty = -1  # penalty for fire\n",
    "        self.goal_reward = 1  # reward for reaching the goal\n",
    "        \n",
    "        self.intended_prob = 0.8  # probability of taking the intended action\n",
    "        self.other_prob = (1 - self.intended_prob) / 3  # probability of taking a random action\n",
    "        \n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]  # available actions\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    # Reset the environment\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "    \n",
    "    # State transition function\n",
    "    def step(self, action):\n",
    "        if self._is_terminal(self.state):\n",
    "            return self.state, 0, True, {}\n",
    "        \n",
    "        i, j = self.state\n",
    "        next_state = self._get_next_state(i, j, action) # get next state based on the action\n",
    "        \n",
    "        reward = self._get_reward(next_state) # get reward based on the next state\n",
    "        \n",
    "        done = self._is_terminal(next_state) # check if the task is completed\n",
    "        \n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    # State transition helper function\n",
    "    def _get_next_state(self, i, j, action):\n",
    "        moves = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
    "        chosen_action = action if np.random.rand() < self.intended_prob else np.random.choice([a for a in self.actions if a != action])\n",
    "        print\n",
    "        di, dj = moves[chosen_action]\n",
    "        next_state = (i + di, j + dj)\n",
    "        \n",
    "        return next_state if self._is_valid_state(next_state) else (i, j)\n",
    "    \n",
    "    # Render the environment\n",
    "    def render(self):\n",
    "        grid = np.full(self.grid_size, 'O', dtype=str)\n",
    "        grid[self.start_state] = 'S'\n",
    "        grid[self.goal_state] = 'G'\n",
    "        grid[self.fire_state] = '!'\n",
    "        grid[self.null_space] = 'X'\n",
    "        \n",
    "        state_i, state_j = self.state\n",
    "        if grid[state_i, state_j] in {'O', '!', 'G'}:\n",
    "            grid[state_i, state_j] = '@'  # Agent's position\n",
    "        \n",
    "        # print(\"+\" + \"---+\" * self.grid_size[1])\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "            # print(\"| \" + \" | \".join(row) + \" |\")\n",
    "            # print(\"+\" + \"---+\" * self.grid_size[1])\n",
    "        print()\n",
    "    \n",
    "    def get_all_states(self):\n",
    "        states = []\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                state = (i, j)\n",
    "                if self._is_valid_state(state):\n",
    "                    states.append(state)\n",
    "        return states\n",
    "    \n",
    "    #  Task completion check\n",
    "    def _is_terminal(self, state):\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    # Environment bound check\n",
    "    def _is_valid_state(self, state):\n",
    "        i, j = state\n",
    "        return 0 <= i < self.grid_size[0] and 0 <= j < self.grid_size[1] and state != self.null_space\n",
    "    \n",
    "    # Reward function\n",
    "    def _get_reward(self, state):  \n",
    "        if state == self.fire_state:\n",
    "            return self.fire_penalty\n",
    "        elif state == self.goal_state:\n",
    "            return self.goal_reward\n",
    "        else:\n",
    "            return self.step_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "S O O O\n",
      "O X O !\n",
      "O O O G\n",
      "\n",
      "Action: down, Reward: 0, Next State: (1, 0)\n",
      "S O O O\n",
      "@ X O !\n",
      "O O O G\n",
      "\n",
      "Action: down, Reward: 0, Next State: (2, 0)\n",
      "S O O O\n",
      "O X O !\n",
      "@ O O G\n",
      "\n",
      "Action: down, Reward: 0, Next State: (2, 0)\n",
      "S O O O\n",
      "O X O !\n",
      "@ O O G\n",
      "\n",
      "Action: down, Reward: 0, Next State: (2, 0)\n",
      "S O O O\n",
      "O X O !\n",
      "@ O O G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = GridWorldEnv()\n",
    "\n",
    "print(\"Initial State:\")\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range (4):\n",
    "    action = \"down\"\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0 -  Action: up, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:1 -  Action: left, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:2 -  Action: down, Reward: 0, Next State: (1, 0)\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "Step:3 -  Action: up, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:4 -  Action: right, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:5 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:6 -  Action: left, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:7 -  Action: right, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:8 -  Action: right, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:9 -  Action: right, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:10 -  Action: down, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:11 -  Action: right, Reward: 0, Next State: (1, 2)\n",
      "S O O O\n",
      "O X A F\n",
      "O O O G\n",
      "\n",
      "Step:12 -  Action: up, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:13 -  Action: up, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:14 -  Action: left, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:15 -  Action: up, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:16 -  Action: right, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:17 -  Action: down, Reward: 0, Next State: (0, 0)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:18 -  Action: right, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:19 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:20 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:21 -  Action: up, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:22 -  Action: right, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:23 -  Action: left, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:24 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:25 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:26 -  Action: left, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:27 -  Action: down, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:28 -  Action: up, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:29 -  Action: right, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:30 -  Action: down, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:31 -  Action: left, Reward: 0, Next State: (0, 1)\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:32 -  Action: right, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:33 -  Action: right, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:34 -  Action: left, Reward: 0, Next State: (0, 2)\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:35 -  Action: right, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:36 -  Action: down, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:37 -  Action: up, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:38 -  Action: up, Reward: 0, Next State: (0, 3)\n",
      "S O O A\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "Step:39 -  Action: down, Reward: -1, Next State: (1, 3)\n",
      "S O O O\n",
      "O X O A\n",
      "O O O G\n",
      "\n",
      "Step:40 -  Action: down, Reward: 1, Next State: (2, 3)\n",
      "S O O O\n",
      "O X O F\n",
      "O O O A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "n = 0\n",
    "while not done:\n",
    "    action = env.actions[np.random.choice(len(env.actions))]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(f\"Step:{n} -  Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "    n += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gibberblot.github.io/rl-notes/single-agent/value-iteration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Step 1: Initialize the value function\n",
    "V = {state: 0 for state in env.get_all_states()}\n",
    "\n",
    "# Step 2: Define hyperparameters\n",
    "gamma = 0.99\n",
    "theta = 1e-6\n",
    "\n",
    "# Step 3: Perform value iteration\n",
    "while True:\n",
    "    delta = 0\n",
    "    \n",
    "    for state in env.get_all_states():\n",
    "        if env.is_terminal(state):\n",
    "            continue\n",
    "        \n",
    "        v = V[state]\n",
    "        max_value = float('-inf')\n",
    "        \n",
    "        for action in env.get_possible_actions(state):\n",
    "            expected_value = sum(prob * (reward + gamma * V[next_state])\n",
    "                                 for next_state, prob, reward in env.get_transitions(state, action))\n",
    "            \n",
    "            max_value = max(max_value, expected_value)\n",
    "        \n",
    "        V[state] = max_value\n",
    "        delta = max(delta, abs(v - V[state]))\n",
    "    \n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "# Step 4: Extract the optimal policy\n",
    "policy = {}\n",
    "\n",
    "for state in env.get_all_states():\n",
    "    if env.is_terminal(state):\n",
    "        continue\n",
    "    \n",
    "    best_action = None\n",
    "    max_value = float('-inf')\n",
    "    \n",
    "    for action in env.get_possible_actions(state):\n",
    "        expected_value = sum(prob * (reward + gamma * V[next_state])\n",
    "                             for next_state, prob, reward in env.get_transitions(state, action))\n",
    "        \n",
    "        if expected_value > max_value:\n",
    "            max_value = expected_value\n",
    "            best_action = action\n",
    "    \n",
    "    policy[state] = best_action\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (0, 0), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 0), Action: down, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 0), Action: left, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 0), Action: right, Next State: (0, 1), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 1), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 1), Action: down, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 1), Action: left, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 1), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 2), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 2), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 2), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 2), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 3), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 3), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 3), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (0, 3), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 0), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 0), Action: down, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 0), Action: left, Next State: (0, 1), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S A O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 0), Action: right, Next State: (0, 2), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O A O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 2), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 2), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 2), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 2), Action: right, Next State: (2, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "A O O G\n",
      "\n",
      "State: (1, 3), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 3), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 3), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (1, 3), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 0), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 0), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 0), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 0), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 1), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 1), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 1), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 1), Action: right, Next State: (2, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "A O O G\n",
      "\n",
      "State: (2, 2), Action: up, Next State: (0, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "O X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 2), Action: down, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 2), Action: left, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "State: (2, 2), Action: right, Next State: (1, 0), Reward: 0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "S O O O\n",
      "A X O F\n",
      "O O O G\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "value_function = np.zeros(env.grid_size)\n",
    "gamma = 0.9\n",
    "\n",
    "n = 0\n",
    "while n < 1:\n",
    "    for state in env.get_all_states():\n",
    "\n",
    "        if env._is_terminal(state):\n",
    "            continue\n",
    "\n",
    "        v = value_function[state]\n",
    "        \n",
    "        next_values = []\n",
    "        for action in env.actions:\n",
    "            next_state, reward, _, _ = env.step(action)\n",
    "            next_values.append(reward + gamma * value_function[next_state])\n",
    "            print(f\"State: {state}, Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "            print(f\"{value_function}\")\n",
    "            env.render()\n",
    "            \n",
    "        value_function[state] = max(next_values)\n",
    "        env.reset()\n",
    "    print(value_function)\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlmlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
